{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c90d55-86d2-44f0-a723-b7ede52958fd",
   "metadata": {},
   "source": [
    "### Neuralink Challenge - Part 1 \n",
    "\n",
    "Generate an exhaustive lookup table of all possible combinations, and a corresponding hash value.\n",
    "\n",
    "Runs in parallel and saves to sqlite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe12d0f1-d7fc-494b-b7b2-546566546d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import wave\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from multiprocessing import Pool, cpu_count, Manager\n",
    "\n",
    "# Function to generate a truncated SHA-256 hash for a given segment\n",
    "def generate_short_hash(segment, hash_size_bytes):\n",
    "    hash_sha256 = hashlib.sha256(segment).digest()[:hash_size_bytes]  # Truncate SHA-256 hash\n",
    "    return hash_sha256.hex()  # Convert to hex string\n",
    "\n",
    "# Function to pack samples into appropriate byte segments\n",
    "def pack_samples(samples, bit_depth):\n",
    "    packed_value = np.uint64(0)\n",
    "    for i, sample in enumerate(samples):\n",
    "        packed_value |= np.uint64(sample) << np.uint64(bit_depth * (len(samples) - i - 1))\n",
    "    num_bytes = (bit_depth * len(samples) + 7) // 8\n",
    "    packed_bytes = packed_value.tobytes()[:num_bytes]  # Convert to bytes\n",
    "    return packed_bytes\n",
    "\n",
    "# Function to save a chunk of the lookup table to the SQLite database\n",
    "def save_chunk_to_db(chunk, db_name, semaphore):\n",
    "    with semaphore:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS lookup (hash TEXT PRIMARY KEY, segment BLOB)''')\n",
    "        c.executemany('INSERT OR IGNORE INTO lookup (hash, segment) VALUES (?, ?)', chunk.items())\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "# Function to generate part of the lookup table\n",
    "def generate_lookup_table_chunk(args):\n",
    "    bit_depth, segment_size, chunk_start, chunk_end, db_name, hash_size_bytes, semaphore, progress_update_interval = args\n",
    "\n",
    "    lookup_table_chunk = {}\n",
    "    num_values_per_sample = 2 ** bit_depth\n",
    "    total_items = chunk_end - chunk_start\n",
    "\n",
    "    def unravel_index(idx, shape):\n",
    "        out = []\n",
    "        for dim in reversed(shape):\n",
    "            out.append(idx % dim)\n",
    "            idx //= dim\n",
    "        return tuple(reversed(out))\n",
    "\n",
    "    for idx in range(chunk_start, chunk_end):\n",
    "        indices = unravel_index(idx, (num_values_per_sample,) * segment_size)\n",
    "        segment = pack_samples(indices, bit_depth)\n",
    "        hash_key = generate_short_hash(segment, hash_size_bytes)\n",
    "        lookup_table_chunk[hash_key] = segment\n",
    "\n",
    "        # Save progress every progress_update_interval\n",
    "        if (idx - chunk_start + 1) % progress_update_interval == 0 or idx == chunk_end - 1:\n",
    "            save_chunk_to_db(lookup_table_chunk, db_name, semaphore)\n",
    "            lookup_table_chunk.clear()\n",
    "\n",
    "# Function to build an exhaustive lookup table for a given segment size using multiple CPUs\n",
    "def build_exhaustive_lookup_table_parallel(bit_depth, segment_size, db_name, hash_size_bytes, num_workers=None, progress_update_interval=0.05):\n",
    "    if num_workers is None:\n",
    "        num_workers = cpu_count()\n",
    "\n",
    "    num_values_per_sample = 2 ** bit_depth\n",
    "    total_combinations = num_values_per_sample ** segment_size\n",
    "    chunk_size = total_combinations // num_workers\n",
    "    update_interval = int(progress_update_interval * chunk_size)\n",
    "\n",
    "    manager = Manager()\n",
    "    semaphore = manager.Semaphore(1)\n",
    "\n",
    "    # Create arguments for each chunk\n",
    "    args = [(bit_depth, segment_size, i * chunk_size, (i + 1) * chunk_size, db_name, hash_size_bytes, semaphore, update_interval) for i in range(num_workers)]\n",
    "    args[-1] = (bit_depth, segment_size, args[-1][2], total_combinations, db_name, hash_size_bytes, semaphore, update_interval)  # Ensure last chunk covers all remaining data\n",
    "\n",
    "    # Use a pool of workers to generate the lookup table in parallel\n",
    "    with Pool(num_workers) as pool:\n",
    "        pool.map(generate_lookup_table_chunk, args)\n",
    "\n",
    "# Function to encode the wav data using the exhaustive lookup table\n",
    "def encode_wav_data_exhaustive(wav_data, bit_depth, segment_size, db_name, hash_size_bytes, semaphore):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    c = conn.cursor()\n",
    "    encoded_data = []\n",
    "    segment_found = []\n",
    "    num_segments = len(wav_data) // segment_size\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        samples = wav_data[i*segment_size:(i+1)*segment_size]\n",
    "        segment = pack_samples(samples, bit_depth)\n",
    "        hash_key = generate_short_hash(segment, hash_size_bytes)\n",
    "        with semaphore:\n",
    "            c.execute('SELECT hash FROM lookup WHERE hash=?', (hash_key,))\n",
    "            result = c.fetchone()\n",
    "        if result:\n",
    "            encoded_data.append(result[0])\n",
    "            segment_found.append(True)\n",
    "        else:\n",
    "            segment_found.append(False)\n",
    "\n",
    "    conn.close()\n",
    "    return encoded_data, segment_found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a2a9e-3e92-4917-b0fc-89832140e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm nl_bit15_seg2_hash3.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b8d18-ad80-433e-b9d7-70b2527d5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "bit_depth = 15\n",
    "segment_size = 2   \n",
    "hash_size_bytes = 3  # Truncated SHA-256 hash size in bytes (3 bytes = 24 bits)\n",
    "db_name = 'nl_bit15_seg2_hash3.db'\n",
    "\n",
    "# Build the exhaustive lookup table in parallel and cache to SQLite\n",
    "build_exhaustive_lookup_table_parallel(bit_depth, \n",
    "                                       segment_size, \n",
    "                                       db_name, hash_size_bytes, progress_update_interval=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c324374-6f4b-467a-afbb-46bbf82b1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_lookup_table(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT COUNT(*) FROM lookup')\n",
    "    count = c.fetchone()[0]\n",
    "    conn.close()\n",
    "    return count\n",
    "\n",
    "count_rows_in_lookup_table(db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6568d-73c6-4f6f-af58-d2045191b185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgb-runtime]",
   "language": "python",
   "name": "conda-env-.conda-xgb-runtime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
